{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Antoine\\miniconda3\\envs\\ML\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "from data_preparation import all_datasets_exist, unpack_datasets, get_datasets, split_datasets, add_dangerous_data, get_test_datasets_only, load_existing_model\n",
    "from training_and_evaluation_functions import train_model, eval_perplexity, save_scores, backup_file\n",
    "from visualization_functions import read_csv_into_matrix, plot_perplexity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 8 LMs to use for prediction\n",
    "1 - untrained small local model\n",
    "2 - everything (small dataset of 2000 random recipes)\n",
    "3 - everything (large dataset of 1M random recipes)\n",
    "4 - All drinks** (Note down how many for the report)\n",
    "5 - All bakery**\n",
    "6 - All meals**\n",
    "7 - First everything, then bakery*\n",
    "8 - ChatGPT\n",
    "\n",
    "*This could be done for meals and drinks, not necessary for the report.\n",
    "**Excludes test\n",
    "\n",
    "## 6 datasets to test on (should have 1000 recipes each)\n",
    "1 - everything mixed\n",
    "2 - drinks\n",
    "3 - bakery\n",
    "4 - meal\n",
    "5 - shuffled recipe steps\n",
    "6 - dangerous steps\n",
    "\"\"\"\n",
    "\n",
    "train_datasets = {\"mixed_subset\": \"train_datasets/mixed_subset.csv\", # 2\n",
    "                 \"all\": \"train_datasets/all.csv\", # 3\n",
    "                 \"drinks\": \"train_datasets/drinks.csv\", # 4\n",
    "                 \"bakery\": \"train_datasets/bakery.csv\", # 5\n",
    "                 \"meal\": \"train_datasets/meal.csv\", # 6\n",
    "                 \"all_but_bakery\": \"train_datasets/all_but_bakery.csv\", # 7         \n",
    "}\n",
    "\n",
    "trained_models = {\"untrained\": \"train_models/untrained/\", # 1\n",
    "                \"mixed_subset\": \"train_models/mixed_subset/\", # 2\n",
    "                 \"all\": \"train_models/all/\", # 3\n",
    "                 \"drinks\": \"train_models/drinks/\", # 4\n",
    "                 \"bakery\": \"train_models/bakery/\", # 5\n",
    "                 \"meal\": \"train_models/meal/\", # 6\n",
    "                 \"everything_then_bakery\": \"train_models/everything_then_bakery/\", # 7  \n",
    "    \n",
    "}\n",
    "\n",
    "test_datasets = {\"all\": \"test_datasets/all.csv\", # 1\n",
    "                 \"drinks\": \"test_datasets/drinks.csv\", # 2\n",
    "                 \"bakery\": \"test_datasets/bakery.csv\", # 3\n",
    "                 \"meal\": \"test_datasets/meal.csv\", # 4\n",
    "                 \"shuffled_steps\": \"test_datasets/shuffled_steps.csv\", # 5\n",
    "                 \"bad\": \"test_datasets/bad.csv\", # 6             \n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents the matrix that displays the results for each combination of trained model and testing set\n",
    "annex = {\"train\": [\"untrained\", \"mixed_subset\", \"all\", \"drinks\", \"bakery\", \"meal\", \"everything_then_bakery\", \"ChatGPT\"],\n",
    "           \"test\": [\"all\", \"drinks\", \"bakery\", \"meal\", \"shuffled_steps\", \"bad\"]}\n",
    "\n",
    "# How does fine-tuning on recipes impact the quality of autocompleted instruction steps?\n",
    "matrix1 = {\"train\": [\"untrained\", \"ChatGPT\", \"mixed_subset\", \"all\", \"drinks\", \"bakery\", \"meal\"],\n",
    "           \"test\": [\"all\", \"drinks\", \"bakery\", \"meal\"]}\n",
    "\n",
    "# For specific categories of recipes, what is the best training strategy?\n",
    "matrix2 = {\"train\": [\"untrained\", \"all\", \"bakery\", \"everything_then_bakery\"],\n",
    "           \"test\": [\"bakery\"]}\n",
    "\n",
    "# Does the fine-tuning process increase or decrease the chance of dangerous instruction steps being generated?  \n",
    "matrix3 = {\"train\": [\"untrained\", \"ChatGPT\", \"mixed_subset\", \"all\", \"bakery\"],\n",
    "           \"test\": [\"all\", \"shuffled_steps\", \"bad\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a bunch of CSV files in train_datasets and test_datasets if they don't all already exist \n",
    "\n",
    "original_dataset = \"full_dataset.csv\"\n",
    "\n",
    "#if not all_datasets_exist(train_datasets, test_datasets):\n",
    "unpack_datasets(train_datasets, test_datasets, original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use existing models\n",
    "Use_existing_models = True\n",
    "\n",
    "# We want to display a matrix of perplexity scores for the two principal features (train_dataset, test_dataset)\n",
    "base_model = \"something\"\n",
    "\n",
    "# For now this order is the only way to know which test sets the perplexity scores are evaluated\n",
    "# If it looks like there's one more category in the data, it's because it's the \"dangerous data\" which is\n",
    "# a different category of dataset. This \"dataset\" takes the last index all the time\n",
    "recipe_categories = [\"main\", \"drinks\", \"bakery\"]\n",
    "\n",
    "saved_perplexity_file = \"saved_perplexity.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we need to write the columns for the csv, they represent on which dataset the perplexity score is calculated on\n",
    "# This is the X axis on the final matrix. Different from the X axis which is training dataset. \n",
    "# Needless to say when X = Y, the test and train data pick from the same category, but different recipes still.\n",
    "csv_rows = [\"recipe_train_dataset\", \"datetime\"] + recipe_categories + [\"bad\"]\n",
    "\n",
    "# Override the existing data\n",
    "with open(saved_perplexity_file, 'w') as file:\n",
    "    pass\n",
    "        \n",
    "with open(saved_perplexity_file, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_rows)\n",
    "    writer.writeheader()  # Write the column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG!!! training model on untrained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Antoine\\miniconda3\\envs\\ML\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring perplexity on test recipe number:  0\n",
      "substep:  0\n",
      "You are a chef-bot autocompleting a small part of a recipe: [START_OF_RECIPE] [RECIPE_TITLE] Petite Lasagna [INGREDIENTS_LIST] [\"5 lasagna noodles (I used 6 and did not use the trimmed pieces)\", \"14 lb ground round\", \"14 lb ground sausage\", \"14 cup chopped onion\", \"1 12 teaspoons minced garlic\", \"12 teaspoon crushed red pepper flakes\", \"1 (4 ounce) can sliced mushrooms, drained\", \"1 12 cups spaghetti sauce\", \"1 egg, slightly beaten\", \"34 cup ricotta cheese\", \"2 tablespoons grated parmesan cheese\", \"2 teaspoons parsley flakes\", \"1 12 teaspoons italian seasoning\", \"1 14 cups shredded Italian cheese blend\", \"fresh ground pepper\", \"kosher salt\"] [STEPS]  1 - Preheat oven \n",
      "substep:  1\n",
      "You are a chef-bot autocompleting a small part of a recipe: [START_OF_RECIPE] [RECIPE_TITLE] Petite Lasagna [INGREDIENTS_LIST] [\"5 lasagna noodles (I used 6 and did not use the trimmed pieces)\", \"14 lb ground round\", \"14 lb ground sausage\", \"14 cup chopped onion\", \"1 12 teaspoons minced garlic\", \"12 teaspoon crushed red pepper flakes\", \"1 (4 ounce) can sliced mushrooms, drained\", \"1 12 cups spaghetti sauce\", \"1 egg, slightly beaten\", \"34 cup ricotta cheese\", \"2 tablespoons grated parmesan cheese\", \"2 teaspoons parsley flakes\", \"1 12 teaspoons italian seasoning\", \"1 14 cups shredded Italian cheese blend\", \"fresh ground pepper\", \"kosher salt\"] [STEPS] 1 - Preheat oven to 350 degrees.  2 - Grease a \n",
      "substep:  2\n",
      "You are a chef-bot autocompleting a small part of a recipe: [START_OF_RECIPE] [RECIPE_TITLE] Petite Lasagna [INGREDIENTS_LIST] [\"5 lasagna noodles (I used 6 and did not use the trimmed pieces)\", \"14 lb ground round\", \"14 lb ground sausage\", \"14 cup chopped onion\", \"1 12 teaspoons minced garlic\", \"12 teaspoon crushed red pepper flakes\", \"1 (4 ounce) can sliced mushrooms, drained\", \"1 12 cups spaghetti sauce\", \"1 egg, slightly beaten\", \"34 cup ricotta cheese\", \"2 tablespoons grated parmesan cheese\", \"2 teaspoons parsley flakes\", \"1 12 teaspoons italian seasoning\", \"1 14 cups shredded Italian cheese blend\", \"fresh ground pepper\", \"kosher salt\"] [STEPS] 1 - Preheat oven to 350 degrees. 2 - Grease a loaf pan (8 or 9 inch).  3 - Cook noodles \n",
      "substep:  3\n",
      "You are a chef-bot autocompleting a small part of a recipe: [START_OF_RECIPE] [RECIPE_TITLE] Petite Lasagna [INGREDIENTS_LIST] [\"5 lasagna noodles (I used 6 and did not use the trimmed pieces)\", \"14 lb ground round\", \"14 lb ground sausage\", \"14 cup chopped onion\", \"1 12 teaspoons minced garlic\", \"12 teaspoon crushed red pepper flakes\", \"1 (4 ounce) can sliced mushrooms, drained\", \"1 12 cups spaghetti sauce\", \"1 egg, slightly beaten\", \"34 cup ricotta cheese\", \"2 tablespoons grated parmesan cheese\", \"2 teaspoons parsley flakes\", \"1 12 teaspoons italian seasoning\", \"1 14 cups shredded Italian cheese blend\", \"fresh ground pepper\", \"kosher salt\"] [STEPS] 1 - Preheat oven to 350 degrees. 2 - Grease a loaf pan (8 or 9 inch). 3 - Cook noodles according to package directions.  4 - While noodles \n",
      "substep:  4\n",
      "You are a chef-bot autocompleting a small part of a recipe: [START_OF_RECIPE] [RECIPE_TITLE] Petite Lasagna [INGREDIENTS_LIST] [\"5 lasagna noodles (I used 6 and did not use the trimmed pieces)\", \"14 lb ground round\", \"14 lb ground sausage\", \"14 cup chopped onion\", \"1 12 teaspoons minced garlic\", \"12 teaspoon crushed red pepper flakes\", \"1 (4 ounce) can sliced mushrooms, drained\", \"1 12 cups spaghetti sauce\", \"1 egg, slightly beaten\", \"34 cup ricotta cheese\", \"2 tablespoons grated parmesan cheese\", \"2 teaspoons parsley flakes\", \"1 12 teaspoons italian seasoning\", \"1 14 cups shredded Italian cheese blend\", \"fresh ground pepper\", \"kosher salt\"] [STEPS] 1 - Preheat oven to 350 degrees. 2 - Grease a loaf pan (8 or 9 inch). 3 - Cook noodles according to package directions. 4 - While noodles are cooking, brown beef, onions, garlic and red pepper flakes over medium heat.  5 - Drain grease \n",
      "substep:  5\n",
      "You are a chef-bot autocompleting a small part of a recipe: [START_OF_RECIPE] [RECIPE_TITLE] Petite Lasagna [INGREDIENTS_LIST] [\"5 lasagna noodles (I used 6 and did not use the trimmed pieces)\", \"14 lb ground round\", \"14 lb ground sausage\", \"14 cup chopped onion\", \"1 12 teaspoons minced garlic\", \"12 teaspoon crushed red pepper flakes\", \"1 (4 ounce) can sliced mushrooms, drained\", \"1 12 cups spaghetti sauce\", \"1 egg, slightly beaten\", \"34 cup ricotta cheese\", \"2 tablespoons grated parmesan cheese\", \"2 teaspoons parsley flakes\", \"1 12 teaspoons italian seasoning\", \"1 14 cups shredded Italian cheese blend\", \"fresh ground pepper\", \"kosher salt\"] [STEPS] 1 - Preheat oven to 350 degrees. 2 - Grease a loaf pan (8 or 9 inch). 3 - Cook noodles according to package directions. 4 - While noodles are cooking, brown beef, onions, garlic and red pepper flakes over medium heat. 5 - Drain grease from pan.  6 - Stir in \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_data \u001b[38;5;129;01min\u001b[39;00m test_datasets:\n\u001b[1;32m---> 25\u001b[0m     perp \u001b[38;5;241m=\u001b[39m \u001b[43meval_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(perp)\n\u001b[0;32m     27\u001b[0m save_scores(name, scores)\n",
      "File \u001b[1;32mc:\\Users\\Antoine\\Desktop\\NLP_project\\project\\Spch-LangProcessingNew\\training_and_evaluation_functions.py:118\u001b[0m, in \u001b[0;36meval_perplexity\u001b[1;34m(trained_model, tokenizer, test_data, bad_data)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mperplexity_across_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Antoine\\Desktop\\NLP_project\\project\\Spch-LangProcessingNew\\training_and_evaluation_functions.py:104\u001b[0m, in \u001b[0;36mperplexity_across_dataset\u001b[1;34m(trained_model, tokenizer, test_df, n_words_before_autocomplete, verbose)\u001b[0m\n\u001b[0;32m    101\u001b[0m prompt \u001b[38;5;241m=\u001b[39m make_prompt(title, ingredients, complete_steps, incomplete_next_step)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[1;32m--> 104\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrest_of_instruction_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m all_perplexity\u001b[38;5;241m.\u001b[39mappend(perplexity)\n\u001b[0;32m    108\u001b[0m complete_steps\u001b[38;5;241m.\u001b[39mappend(steps[j])\n",
      "File \u001b[1;32mc:\\Users\\Antoine\\Desktop\\NLP_project\\project\\Spch-LangProcessingNew\\training_and_evaluation_functions.py:72\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[1;34m(trained_model, tokenizer, original_prompt, rest_of_instruction_step)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Calculate log likelihood\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 72\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     log_likelihood \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m completion_id\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Total log likelihood\u001b[39;00m\n\u001b[0;32m     75\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_likelihood \u001b[38;5;241m/\u001b[39m completion_id\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Antoine\\miniconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Antoine\\miniconda3\\envs\\ML\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1098\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1096\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1098\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Antoine\\miniconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Antoine\\miniconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# datasets = get_datasets(recipe_categories=recipe_categories)\n",
    "# train_datasets, test_datasets = split_datasets(datasets)\n",
    "\n",
    "# train_datasets = [None] + train_datasets\n",
    "dataset_names = [\"untrained\"] + recipe_categories\n",
    "\n",
    "\n",
    "\n",
    "test_datasets = get_test_datasets_only(recipe_categories)\n",
    "# test_datasets = add_dangerous_data(test_datasets)\n",
    "\n",
    "for train_data, name in zip(dataset_names, dataset_names):\n",
    "    print(f\"OMG!!! training model on {name}\")\n",
    "    if Use_existing_models:\n",
    "        trained_model, tokenizer = load_existing_model(name)\n",
    "    else: \n",
    "        #if name == \"untrained\":\n",
    "        #    trained_model = base_model\n",
    "        #else: \n",
    "        #    trained_model = train_model(base_model, train_datasets)\n",
    "        pass\n",
    "    \n",
    "    scores = []\n",
    "    for test_data in test_datasets:\n",
    "        perp = eval_perplexity(trained_model, tokenizer, test_data)\n",
    "        scores.append(perp)\n",
    "    save_scores(name, scores)\n",
    "    \n",
    "backup_file(saved_perplexity_file, \"saved_perplexities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, Y_axis, X_axis = read_csv_into_matrix(saved_perplexity_file)\n",
    "plot_perplexity_matrix(matrix, Y_axis, X_axis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
